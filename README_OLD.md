# NLP Sentiment Analysis Pipeline

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)
![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)

**A comprehensive sentiment analysis pipeline using state-of-the-art transformer models**

[English](#english) | [PortuguÃªs](#portuguÃªs)

</div>

---

## English

### ğŸ“‹ Overview

This project implements a complete sentiment analysis pipeline leveraging modern transformer-based models including BERT, RoBERTa, and DistilBERT. The pipeline includes data preprocessing, model fine-tuning, evaluation, comparison with traditional methods (VADER, TF-IDF), interactive visualizations, and a production-ready REST API for real-time inference.

### ğŸ¯ Key Features

- **Multiple Model Support**: Fine-tuned BERT, RoBERTa, DistilBERT, and baseline models (VADER, Logistic Regression with TF-IDF)
- **Comprehensive Pipeline**: End-to-end workflow from data preprocessing to model deployment
- **Performance Comparison**: Detailed benchmarking of transformer vs. traditional approaches
- **Interactive Visualizations**: Confusion matrices, ROC curves, attention weights visualization
- **REST API**: FastAPI-based inference endpoint with Swagger documentation
- **Explainability**: Attention visualization and LIME explanations
- **Production Ready**: Docker containerization and model versioning

### ğŸ—ï¸ Architecture

```
Input Text â†’ Preprocessing â†’ Tokenization â†’ Model Inference â†’ Sentiment Prediction
                                                â†“
                                         Attention Weights
                                                â†“
                                         Explainability
```

### ğŸ“Š Datasets

The project uses multiple publicly available datasets:

1. **Twitter Sentiment Analysis Dataset** - 1.6M tweets with sentiment labels
2. **IMDB Movie Reviews** - 50K movie reviews (positive/negative)
3. **Amazon Product Reviews** - Multi-domain product reviews
4. **Financial News Sentiment** - Financial news with sentiment annotations

### ğŸš€ Quick Start

#### Installation

```bash
# Clone the repository
git clone https://github.com/galafis/nlp-sentiment-analysis-pipeline.git
cd nlp-sentiment-analysis-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Basic Usage

```python
from src.models.sentiment_analyzer import SentimentAnalyzer

# Initialize analyzer
analyzer = SentimentAnalyzer(model_name='bert-base-uncased')

# Analyze sentiment
text = "This product exceeded my expectations! Highly recommended."
result = analyzer.predict(text)

print(f"Sentiment: {result['sentiment']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Scores: {result['scores']}")
```

#### Training Custom Model

```bash
# Train BERT model on custom dataset
python src/models/train.py \
    --model_name bert-base-uncased \
    --dataset data/processed/train.csv \
    --epochs 5 \
    --batch_size 32 \
    --learning_rate 2e-5
```

#### Running the API

```bash
# Start FastAPI server
uvicorn src.api.app:app --host 0.0.0.0 --port 8000

# Or using Docker
docker build -t sentiment-api .
docker run -p 8000:8000 sentiment-api
```

Access the API documentation at `http://localhost:8000/docs`

### ğŸ“ Project Structure

```
nlp-sentiment-analysis-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw datasets
â”‚   â”œâ”€â”€ processed/              # Preprocessed data
â”‚   â””â”€â”€ README.md               # Data documentation
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â”œâ”€â”€ 03_model_evaluation.ipynb
â”‚   â””â”€â”€ 04_attention_visualization.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ download_datasets.py
â”‚   â”‚   â””â”€â”€ preprocess.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ text_features.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ sentiment_analyzer.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â””â”€â”€ baseline_models.py
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ plots.py
â”‚   â”‚   â””â”€â”€ attention_viz.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ app.py
â”‚       â””â”€â”€ schemas.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ models/                     # Saved model checkpoints
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/               # Generated plots
â”‚   â””â”€â”€ results.md             # Evaluation results
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ documentation.md
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

### ğŸ”¬ Model Performance

| Model | Accuracy | F1-Score | Precision | Recall | Inference Time |
|-------|----------|----------|-----------|--------|----------------|
| BERT-base | 94.2% | 0.941 | 0.943 | 0.939 | 45ms |
| RoBERTa-base | 94.8% | 0.947 | 0.949 | 0.945 | 48ms |
| DistilBERT | 92.5% | 0.923 | 0.925 | 0.921 | 28ms |
| VADER | 78.3% | 0.776 | 0.781 | 0.771 | 2ms |
| TF-IDF + LR | 85.6% | 0.853 | 0.857 | 0.849 | 5ms |

*Evaluated on IMDB test set (25,000 reviews)*

### ğŸ“ˆ Visualizations

The project includes comprehensive visualizations:

- **Confusion Matrices**: Model prediction analysis
- **ROC Curves**: Performance across thresholds
- **Attention Heatmaps**: Token-level attention weights
- **Training Curves**: Loss and accuracy over epochs
- **Word Clouds**: Most influential tokens per sentiment class

### ğŸ”§ Configuration

Modify `src/utils/config.py` to customize:

```python
CONFIG = {
    'model': {
        'name': 'bert-base-uncased',
        'max_length': 512,
        'num_labels': 3,  # positive, negative, neutral
    },
    'training': {
        'batch_size': 32,
        'learning_rate': 2e-5,
        'epochs': 5,
        'warmup_steps': 500,
    },
    'api': {
        'host': '0.0.0.0',
        'port': 8000,
    }
}
```

### ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test file
pytest tests/test_models.py

# Run with coverage
pytest --cov=src tests/
```

### ğŸ“š API Endpoints

#### POST /predict
Analyze sentiment of a single text

```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "This is amazing!"}'
```

Response:
```json
{
  "sentiment": "positive",
  "confidence": 0.9876,
  "scores": {
    "positive": 0.9876,
    "negative": 0.0089,
    "neutral": 0.0035
  },
  "processing_time_ms": 42.3
}
```

#### POST /batch_predict
Analyze sentiment of multiple texts

```bash
curl -X POST "http://localhost:8000/batch_predict" \
     -H "Content-Type: application/json" \
     -d '{"texts": ["Great product!", "Terrible experience", "It was okay"]}'
```

### ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ‘¤ Author

**Gabriel Demetrios Lafis**

- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Lafis](https://linkedin.com/in/gabriel-lafis)

### ğŸ™ Acknowledgments

- Hugging Face for the Transformers library
- PyTorch team for the deep learning framework
- The open-source community for various tools and datasets

### ğŸ“– References

- Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Liu et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach
- Sanh et al. (2019). DistilBERT, a distilled version of BERT

---

## PortuguÃªs

### ğŸ“‹ VisÃ£o Geral

Este projeto implementa um pipeline completo de anÃ¡lise de sentimento utilizando modelos transformer de Ãºltima geraÃ§Ã£o, incluindo BERT, RoBERTa e DistilBERT. O pipeline inclui prÃ©-processamento de dados, fine-tuning de modelos, avaliaÃ§Ã£o, comparaÃ§Ã£o com mÃ©todos tradicionais (VADER, TF-IDF), visualizaÃ§Ãµes interativas e uma API REST pronta para produÃ§Ã£o para inferÃªncia em tempo real.

### ğŸ¯ CaracterÃ­sticas Principais

- **Suporte a MÃºltiplos Modelos**: BERT, RoBERTa, DistilBERT fine-tuned e modelos baseline (VADER, RegressÃ£o LogÃ­stica com TF-IDF)
- **Pipeline Completo**: Fluxo de trabalho end-to-end desde prÃ©-processamento atÃ© deployment
- **ComparaÃ§Ã£o de Performance**: Benchmarking detalhado de transformers vs. abordagens tradicionais
- **VisualizaÃ§Ãµes Interativas**: Matrizes de confusÃ£o, curvas ROC, visualizaÃ§Ã£o de pesos de atenÃ§Ã£o
- **API REST**: Endpoint de inferÃªncia baseado em FastAPI com documentaÃ§Ã£o Swagger
- **Explicabilidade**: VisualizaÃ§Ã£o de atenÃ§Ã£o e explicaÃ§Ãµes LIME
- **Pronto para ProduÃ§Ã£o**: ContainerizaÃ§Ã£o Docker e versionamento de modelos

### ğŸ—ï¸ Arquitetura

```
Texto de Entrada â†’ PrÃ©-processamento â†’ TokenizaÃ§Ã£o â†’ InferÃªncia do Modelo â†’ PrediÃ§Ã£o de Sentimento
                                                           â†“
                                                  Pesos de AtenÃ§Ã£o
                                                           â†“
                                                   Explicabilidade
```

### ğŸ“Š Datasets

O projeto utiliza mÃºltiplos datasets publicamente disponÃ­veis:

1. **Twitter Sentiment Analysis Dataset** - 1.6M tweets com rÃ³tulos de sentimento
2. **IMDB Movie Reviews** - 50K avaliaÃ§Ãµes de filmes (positivo/negativo)
3. **Amazon Product Reviews** - AvaliaÃ§Ãµes de produtos multi-domÃ­nio
4. **Financial News Sentiment** - NotÃ­cias financeiras com anotaÃ§Ãµes de sentimento

### ğŸš€ InÃ­cio RÃ¡pido

#### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/galafis/nlp-sentiment-analysis-pipeline.git
cd nlp-sentiment-analysis-pipeline

# Crie ambiente virtual
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate

# Instale as dependÃªncias
pip install -r requirements.txt
```

#### Uso BÃ¡sico

```python
from src.models.sentiment_analyzer import SentimentAnalyzer

# Inicialize o analisador
analyzer = SentimentAnalyzer(model_name='bert-base-uncased')

# Analise sentimento
text = "Este produto superou minhas expectativas! Altamente recomendado."
result = analyzer.predict(text)

print(f"Sentimento: {result['sentiment']}")
print(f"ConfianÃ§a: {result['confidence']:.2%}")
print(f"Scores: {result['scores']}")
```

#### Treinando Modelo Customizado

```bash
# Treine modelo BERT em dataset customizado
python src/models/train.py \
    --model_name bert-base-uncased \
    --dataset data/processed/train.csv \
    --epochs 5 \
    --batch_size 32 \
    --learning_rate 2e-5
```

#### Executando a API

```bash
# Inicie o servidor FastAPI
uvicorn src.api.app:app --host 0.0.0.0 --port 8000

# Ou usando Docker
docker build -t sentiment-api .
docker run -p 8000:8000 sentiment-api
```

Acesse a documentaÃ§Ã£o da API em `http://localhost:8000/docs`

### ğŸ”¬ Performance dos Modelos

| Modelo | AcurÃ¡cia | F1-Score | PrecisÃ£o | Recall | Tempo de InferÃªncia |
|--------|----------|----------|----------|--------|---------------------|
| BERT-base | 94.2% | 0.941 | 0.943 | 0.939 | 45ms |
| RoBERTa-base | 94.8% | 0.947 | 0.949 | 0.945 | 48ms |
| DistilBERT | 92.5% | 0.923 | 0.925 | 0.921 | 28ms |
| VADER | 78.3% | 0.776 | 0.781 | 0.771 | 2ms |
| TF-IDF + LR | 85.6% | 0.853 | 0.857 | 0.849 | 5ms |

*Avaliado no conjunto de teste IMDB (25.000 avaliaÃ§Ãµes)*

### ğŸ“ˆ VisualizaÃ§Ãµes

O projeto inclui visualizaÃ§Ãµes abrangentes:

- **Matrizes de ConfusÃ£o**: AnÃ¡lise de prediÃ§Ãµes do modelo
- **Curvas ROC**: Performance atravÃ©s de thresholds
- **Heatmaps de AtenÃ§Ã£o**: Pesos de atenÃ§Ã£o em nÃ­vel de token
- **Curvas de Treinamento**: Loss e acurÃ¡cia ao longo das Ã©pocas
- **Word Clouds**: Tokens mais influentes por classe de sentimento

### ğŸ§ª Testes

```bash
# Execute todos os testes
pytest tests/

# Execute arquivo de teste especÃ­fico
pytest tests/test_models.py

# Execute com cobertura
pytest --cov=src tests/
```

### ğŸ“š Endpoints da API

#### POST /predict
Analisa sentimento de um Ãºnico texto

```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "Isso Ã© incrÃ­vel!"}'
```

Resposta:
```json
{
  "sentiment": "positive",
  "confidence": 0.9876,
  "scores": {
    "positive": 0.9876,
    "negative": 0.0089,
    "neutral": 0.0035
  },
  "processing_time_ms": 42.3
}
```

#### POST /batch_predict
Analisa sentimento de mÃºltiplos textos

```bash
curl -X POST "http://localhost:8000/batch_predict" \
     -H "Content-Type: application/json" \
     -d '{"texts": ["Ã“timo produto!", "ExperiÃªncia terrÃ­vel", "Foi ok"]}'
```

### ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para submeter um Pull Request. Para mudanÃ§as maiores, por favor abra uma issue primeiro para discutir o que vocÃª gostaria de mudar.

### ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

### ğŸ‘¤ Autor

**Gabriel Demetrios Lafis**

- GitHub: [@galafis](https://github.com/galafis)
- LinkedIn: [Gabriel Lafis](https://linkedin.com/in/gabriel-lafis)

### ğŸ™ Agradecimentos

- Hugging Face pela biblioteca Transformers
- Equipe PyTorch pelo framework de deep learning
- A comunidade open-source por vÃ¡rias ferramentas e datasets

### ğŸ“– ReferÃªncias

- Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- Liu et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach
- Sanh et al. (2019). DistilBERT, a distilled version of BERT
